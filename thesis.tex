%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Thin Sectioned Essay
% LaTeX Template
% Version 1.0 (3/8/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original Author:
% Nicolas Diaz (nsdiaz@uc.cl) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper, 11pt]{article} % Font size (can be 10pt, 11pt or 12pt) and paper size (remove a4paper for US letter paper)

\usepackage[protrusion=true,expansion=true]{microtype} % Better typography
\usepackage{graphicx} % Required for including pictures
\usepackage{wrapfig} % Allows in-line images
\usepackage{amsmath, amsfonts}

\usepackage{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Required for accented characters
\linespread{1.05} % Change line spacing here, Palatino benefits from a slight increase by default

\makeatletter
\renewcommand\@biblabel[1]{\textbf{#1.}} % Change the square brackets for each bibliography item from '[1]' to '1.'
\renewcommand{\@listI}{\itemsep=0pt} % Reduce the space between items in the itemize and enumerate environments and the bibliography

\renewcommand{\maketitle}{ % Customize the title - do not edit title and author name here, see the TITLE block below
\begin{flushright} % Right align
{\LARGE\@title} % Increase the font size of the title

\vspace{50pt} % Some vertical space between the title and author name

{\large\@author} % Author name
\\\@date % Date

\vspace{40pt} % Some vertical space between the author block and abstract
\end{flushright}
}

%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------

\title{\textbf{Incrementally Interpreting Multimodal Referring Expressions in Real Time}} % Subtitle

\author{\textsc{Miles Eldon}\\ % Author
\textsc{Professor Stefanie Tellex} (Reader and Advisor)\\
{\textsc{Professor Michael Littman} (Second Reader)}
\\{\textit{Brown University}}} % Institution

\date{\today} % Date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title section

\section{Introduction}
Robots have long been a great asset in factory settings, yet remain unable to make the crucial leap into daily life, interacting in a variety of ways with human users. A large barrier to this leap has been an inability to quickly understand and react to the varying requests human users can supply in rapidly changing environments. My work focuses on creating the capability for a robotic assistant to understand and respond to a human user's referring expressions in real time, allowing for fluid and meaningful interactions. Previous work has focused on understanding human requests, but is limited to single modalities such as speech. There have been several systems that fused multiple modalities to create a more robust system, but even these have been performed in a offline manner, creating an unreasonable processing delay before the robotic assistant can react to user requests. My work focuses on maintaining a distribution over a user's state at all time to allow for continuous recognition and response. By continuously computing the belief state using a Bayes Filter, my system can continuously estimate the true user state while processing multiple sources of input, such as speech and gesture. The Bayes Filter approach provides an accurate way of measuring the current user's state, but limits the ability to respond. To extend the Bayes Filter to a model that allows for meaningful responses, my system is modified into a Partially Observable Markov Decision Process (POMDP). Although computation of the POMDP I propose is intractable, I also provide a reasonable approximation of the POMDP, which can be estimated in real time, providing rapid and accurate action choices in real time. The result is a system capable of interpreting and responding to human requests, either by acting or requesting clarification, in a variety of scenarios. For this thesis, the scenario is limited to requesting objects from a table using the Baxter Robot, but this can easily be extended with the encoding of the appropriate observations and actions.
\section{Assumptions and Their Implications}
The Bayes Filter and POMDP share several assumptions. They are that:
\begin{itemize}
\item The human desire for objects to be handed to them is Markovian. The results of the user studies justify this as a fairly reasonable simplifying assumption.
\item The system knows all objects that might be considered in our environment. This is to facilitate object recognition and model generation. This is currently necessary because I do not have a system that can quickly and reliably train object models on the fly, but would not be difficult to change is such a system became available.
\item The forearm (as opposed to the fingers) creates a pointing gesture. This reduces accuracy and is not ideal, but was necessitated by a lack of a reliable hand tracker at the distance I required. Again, this can be easily changed if a reliable hand tracking system was acquired.
\item The user always wants the robot to hand them an object. This can by mitigated by adding a \textit{null} state, where the robot believes the user does not want anything. In application, unless there is only one object in the scene, the results are identical either way, and so I chose not to incorporate such a state for simplicity.
\item All parts of observations (discussed in the next section), are conditionally independent. Namely, given that a user wants a specific object $x$, the probability that they point at $x$ with their left hand and that they point at $z$ with their right hand are independent. This makes the computation of $p(z|x)$ tractable, and, as the user studies show, is a reasonable assumption.
\end{itemize}
%This system assumes that \\
%This system assumes that \\
%This system assume that the 
\section{The Bayes Filter}
The aim of the Bayes Filter is to estimate a distribution over the object in a scene that a user is referring to given language and gesture inputs. We wish to estimate the user state $x \in \mathcal{X}$, the object in the scene the person is currently referring to. The Bayes Filter assumes that the users state behaves in a  Markovian way, such that each state depends only on the previous state, and each observation depends only on the state that produced it:

\includegraphics[scale=0.7]{images/bayes_filter}

\subsection{Technical Approach}
The robot observes the user's body pose and speech $\mathcal{Z}$, and at each time step estimates a distribution over the current state, $x_t$:
\begin{align}
p(x_t | z_0, \dots, z_t)
\end{align}

To estimate this distribution, we perform alternating time and measurement updates. The time update incorporates the hidden state transitions using the previous state estimate and knowledge of the state transitions:
\begin{align}
p(x_t | z_{0:t-1}) = \int_{x_{t-1} \in \mathcal{X}} p(x_t|x_{t-1})\times p(x_{t-1} | z_{0:t-1}) \text{d}x_{t-1}
\end{align}

The measurement update combines the previous belief with the newest observation to update each belief state: 
\begin{align}
p(x_t |z_{0:t}) = \frac{p(z_t | x_t) \times p(x_t | z_{0:t-1})}{p(z_t | z_{0:t-1})} \\\propto p(z_t | x_t) \times p(x_t | z_{0:t-1})
\end{align}

This gives us an easily calculable estimation of the users state at each time step.
\subsection{Prediction Model}
We assume that a person is likely to continue referring to the same
object, and so has a large probability $c$ of remaining in the state

\begin{align}
p(x_t | x_{t-1}) = \left\{  \begin{array}{ll}
c&\mbox{if } x_t = x_{t-1}\\
1-\frac{c}{|X|-1}&\mbox{otherwise}
\end{array}\right.
\end{align}

This assumption means that the robot's certainty slowly decays over
time, in the absence of corroborating information, converging to a
uniform distribution.  It enables our framework to integrate past
language and gesture information but also quickly adapt to new,
conflicting information because it assumes the person has changed
objects.
\subsection{Observation Model}
We assume access to an observation model of the form:
\begin{align}
p(z_t | x_t)
\end{align}

Observations consist of a tuple consisting of a person's actions,
$\langle l, r, h, s\rangle $ where:
\begin{itemize}
	\item $l$ represents a vector from the observed origin ($l_o$) to the point ($l_v$) of the left arm.
	\item $r$ represents a vector from the observed origin ($r_o$) to the point ($r_v$) of the right arm.
	\item $h$ represents a vector from the observed origin ($h_o$) to the point ($h_v$) of the head. This is the angle of the head transformed into a unit length vector.
	\item $s$ represents the observed speech from the user,
          consisting of a list of words.
	\end{itemize}

Formally, we have:
Formally, we have:
\begin{align}
p(z_t | x_t) &= p(l, r, h, s | x_t)\\
\intertext{We factor assuming that each modality is independent of the others given the state (the true object that the person is referencing):}
p(z_t | x_t) &= p(l | x_t) \times p(r | x_t) \times p(h | x_t) \times p(s | x_t)
\end{align}

\noindent The following sections describe how we model each type of
input from the person.  We track body pose using the NITE
Tracker and define our models for the arm and head
gestures in terms of the person's body pose.\\\\
\noindent{\bf Gesture.}  We model pointing gestures as a vector
through three dimensional space.  First, we calculate a gesture vector
using the skeleton pose returned by NITE.  For arms, we compute a
vector from the elbow to the wrist, then project this vector so that the origin is at the wrist.  For head pose, we compute a
vector based on the body orientation. Next, we calculate the angle between the gesture
vector and the vector from the gesture origin to the center of each object, and then use the PDF of a Gaussian ($\mathcal{N}$) with
variance ($\sigma$) to determine the weight that should be assigned to
that object. We define a function $\mbox{A}(o, p_1, p_2)$ as the angle
between the two points, $p_1$ and $p_2$ with the given origin, $o$.
Then
\begin{align}
p(l | x_t) \propto \mathcal{N}(\mu_l=0, \sigma_l)[A(l_o, l_v, x_t)]\\
p(r | x_t) \propto \mathcal{N}(\mu_r=0, \sigma_r)[A(r_o, r_v, x_t)]
\end{align}

If the person's arm is more than a certain angle away from the table,
we assume they are referring to none of the objects, and perform an
update.  As a result, these gestures do not effect the robot's
estimate of the objects being referenced.\\\\
\noindent{\bf Head Pose.}
Head pose is modeled in the same manner as arm gestures.
\begin{align}
p(h | x_t) \propto \mathcal{N}(\mu_h=0, \sigma_h)[A(h_o, h_v, x_t)]
\end{align}


\noindent{\bf Speech.}  We model speech with a unigram model, namely we
take each word in a given speech input and calcuate the probability that, given the state, that word would have been spoken.
\begin{align}
p(s |x_t) = \displaystyle \prod_{w \in s} p(w | x_t)
\end{align}
\subsubsection{Null Words and Gesture}

To account for continuous gesture and non-continuous speech input, we
have both null poses and speech.  When no words are spoken, we assume
a null word which has a uniform distribution over the objects.  This
effect means that spoken words cause a discrete bump in probability
according to the language model, which then decays over time. While
gesture remains a continuous input throughout the entire interaction,
many gestures have little or no meaning. To allow for these without
overloading the model with noise, we also calculate the angle between
each arm vector and each foot. If the angle between the arms and a
foot is smaller than the angle between the arms and any object, we
assume that the user is in a resting pose, and treat that gesture as
indicating uniform probability over all states.
\subsubsection{Model Parameters}
We tuned model parameters by hand.  We considered collecting and
annotating a data set to train the model parameters, but we found our
initial process to be quite accurate. We generated the language model
by hand, adding to it based on results of our pilot
studies. After our initial tuning, we fixed
  model parameters, and results reported in the paper all use the same
  fixed set of parameters. We expect that as we add
larger sets of objects, a language model trained using data from
Amazon Mechanical Turk or other corpora will be necessary to increase
robustness over a larger set of objects.  

In our experiments, we had the following parameters: the transition
probability, $c$, was 0.0005. We set this parameter to give an object
that has 100\% confidence an approximately 10\% drop in confidence per
second with all null observations.  Standard deviation for the
Gaussian used to model probability of gesture, $\sigma_l$, $\sigma_r$,
and $\sigma_h$ was 1.0 radians. We found that this standard deviation
allowed for accurate pointing, without skewing the probabilities
during an arm swing.  The language model consisted of 16 unique words,
containing common descriptors for the objects such as ``bowl,''
``spoon,'' ``metal,'' ``shiny,'' etc. It also included words that were
commonly misinterpreted by the speech recognition system, such as
``bull'' when the user was requesting a bowl.

\subsection*{User Studies}
\subsection*{Conclusions}
\end{document}