%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Thin Sectioned Essay
% LaTeX Template
% Version 1.0 (3/8/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original Author:
% Nicolas Diaz (nsdiaz@uc.cl) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper, 11pt]{article} % Font size (can be 10pt, 11pt or 12pt) and paper size (remove a4paper for US letter paper)
\usepackage[ruled]{algorithm2e}
\usepackage[protrusion=true,expansion=true]{microtype} % Better typography
\usepackage{graphicx} % Required for including pictures
\usepackage{wrapfig} % Allows in-line images
\usepackage{amsmath, amsfonts}
\usepackage{hyperref}
\usepackage{subfigure}
\usepackage{booktabs}


\usepackage{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Required for accented characters
\linespread{1.05} % Change line spacing here, Palatino benefits from a slight increase by default

\makeatletter
\renewcommand\@biblabel[1]{\textbf{#1.}} % Change the square brackets for each bibliography item from '[1]' to '1.'
\renewcommand{\@listI}{\itemsep=0pt} % Reduce the space between items in the itemize and enumerate environments and the bibliography

\renewcommand{\maketitle}{ % Customize the title - do not edit title and author name here, see the TITLE block below
\begin{flushright} % Right align
{\LARGE\@title} % Increase the font size of the title

\vspace{50pt} % Some vertical space between the title and author name

{\large\@author} % Author name
\\\@date % Date

\vspace{40pt} % Some vertical space between the author block and abstract
\end{flushright}
}

%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------

\title{\textbf{Incrementally Interpreting Multimodal Referring Expressions in Real Time}} % Subtitle

\author{\textsc{Miles Eldon}\\ % Author
\textsc{Professor Stefanie Tellex} (Reader and Advisor)\\
{\textsc{Professor Michael Littman} (Second Reader)}
\\{\textit{Brown University}}} % Institution

\date{\today} % Date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title section

\section{Introduction}
Robots have long been a great asset in factory settings, yet remain unable to make the crucial leap into daily life, interacting in a variety of ways with human users. A large barrier to this leap has been an inability to quickly understand and react to the varying requests human users can supply in rapidly changing environments. My work focuses on creating the capability for a robotic assistant to understand and respond to a human user's referring expressions in real time, allowing for fluid and meaningful interactions. Previous work has focused on understanding human requests, but is limited to single modalities such as speech. There have been several systems that fused multiple modalities to create a more robust system, but even these have been performed in a offline manner, creating an unreasonable processing delay before the robotic assistant can react to user requests. My work focuses on maintaining a distribution over a user's state at all time to allow for continuous recognition and response. By continuously computing the belief state using a Bayes Filter, my system can estimate the true user state while processing multiple sources of input, such as speech and gesture in real time.
%To extend the Bayes Filter to a model that allows for meaningful responses, my system is modified into a Partially Observable Markov Decision Process (POMDP). Although computation of the POMDP I propose is intractable, I also provide a reasonable approximation of the POMDP, which can be estimated in real time, providing rapid and accurate action choices in real time. The result is a system capable of interpreting and responding to human requests, either by acting or requesting clarification, in a variety of scenarios. For this thesis, the scenario is limited to requesting objects from a table using the Baxter Robot, but this can easily be extended with the encoding of the appropriate observations and actions.
\section{Assumptions and Their Implications}
The Bayes Filter makes several assumptions. They are that:
\begin{itemize}
\item The human desire for objects to be handed to them is Markovian. The results of the user studies justify this as a fairly reasonable simplifying assumption.
\item The system knows all objects that might be considered in our environment. This is to facilitate object recognition and model generation. This is currently necessary because I do not have a system that can quickly and reliably train object models on the fly, but would not be difficult to change is such a system became available.
\item The forearm (as opposed to the fingers) creates a pointing gesture. This reduces accuracy and is not ideal, but was necessitated by a lack of a reliable hand tracker at the distance I required. Again, this can be easily changed if a reliable hand tracking system was acquired.
\item The user always wants the robot to hand them an object. This can by mitigated by adding a \textit{null} state, where the robot believes the user does not want anything. In application, unless there is only one object in the scene, the results are identical either way, and so I chose not to incorporate such a state for simplicity.
\item All parts of observations (discussed in the next section), are conditionally independent. Namely, given that a user wants a specific object $x$, the probability that they point at $x$ with their left hand and that they point at $z$ with their right hand are independent. This makes the computation of $p(z|x)$ tractable, and, as the user studies show, is a reasonable assumption.
\end{itemize}
%This system assumes that \\
%This system assumes that \\
%This system assume that the 
\section{The Bayes Filter}
The aim of the Bayes Filter is to estimate a distribution over the object in a scene that a user is referring to given language and gesture inputs. We wish to estimate the user state $x \in \mathcal{X}$, the object in the scene the person is currently referring to. The Bayes Filter assumes that the users state behaves in a  Markovian way, such that each state depends only on the previous state, and each observation depends only on the state that produced it:

\includegraphics[scale=0.7]{images/bayes_filter}

\section{Technical Approach}
The robot observes the user's body pose and speech $\mathcal{Z}$, and at each time step estimates a distribution over the current state, $x_t$:
\begin{align}
p(x_t | z_0, \dots, z_t)
\end{align}

To estimate this distribution, we perform alternating time and measurement updates. The time update incorporates the hidden state transitions using the previous state estimate and knowledge of the state transitions:
\begin{align}
p(x_t | z_{0:t-1}) = \int_{x_{t-1} \in \mathcal{X}} p(x_t|x_{t-1})\times p(x_{t-1} | z_{0:t-1}) \text{d}x_{t-1}
\end{align}

The measurement update combines the previous belief with the newest observation to update each belief state: 
\begin{align}
p(x_t |z_{0:t}) = \frac{p(z_t | x_t) \times p(x_t | z_{0:t-1})}{p(z_t | z_{0:t-1})} \\\propto p(z_t | x_t) \times p(x_t | z_{0:t-1})
\end{align}

This gives us an easily calculable estimation of the users state at each time step.
\subsection{Prediction Model}
We assume that a person is likely to continue referring to the same
object, and so has a large probability $c$ of remaining in the state

\begin{align}
p(x_t | x_{t-1}) = \left\{  \begin{array}{ll}
c&\mbox{if } x_t = x_{t-1}\\
1-\frac{c}{|X|-1}&\mbox{otherwise}
\end{array}\right.
\end{align}

This assumption means that the robot's certainty slowly decays over
time, in the absence of corroborating information, converging to a
uniform distribution.  It enables our framework to integrate past
language and gesture information but also quickly adapt to new,
conflicting information because it assumes the person has changed
objects.
\subsection{Observation Model}
We assume access to an observation model of the form:
\begin{align}
p(z_t | x_t)
\end{align}

Observations consist of a tuple consisting of a person's actions,
$\langle l, r, h, s\rangle $ where:
\begin{itemize}
	\item $l$ represents a vector from the observed origin ($l_o$) to the point ($l_v$) of the left arm.
	\item $r$ represents a vector from the observed origin ($r_o$) to the point ($r_v$) of the right arm.
	\item $h$ represents a vector from the observed origin ($h_o$) to the point ($h_v$) of the head. This is the angle of the head transformed into a unit length vector.
	\item $s$ represents the observed speech from the user,
          consisting of a list of words.
	\end{itemize}

Formally, we have:
\begin{align}
p(z_t | x_t) &= p(l, r, h, s | x_t)\\
\intertext{We factor assuming that each modality is conditionally independent of the others given the state (the true object that the person is referencing):}
p(z_t | x_t) &= p(l | x_t) \times p(r | x_t) \times p(h | x_t) \times p(s | x_t)
\end{align}

\noindent The following sections \(  \)¥escribe how we model each type of
input from the person.  We track body pose using the NITE
Tracker and define our models for the arm and head
gestures in terms of the person's body pose.\\\\
\noindent{\bf Gesture}

We model pointing gestures as a vector
through three dimensional space.  First, we calculate a gesture vector
using the skeleton pose returned by NITE.  For arms, we compute a
vector from the elbow to the wrist, then project this vector so that the origin is at the wrist.  For head pose, we compute a
vector based on the body orientation. Next, we calculate the angle between the gesture
vector and the vector from the gesture origin to the center of each object, and then use the PDF of a Gaussian ($\mathcal{N}$) with
variance ($\sigma$) to determine the weight that should be assigned to
that object. We define a function $\mbox{A}(o, p_1, p_2)$ as the angle
between the two points, $p_1$ and $p_2$ with the given origin, $o$.
Then
\begin{align}
p(l | x_t) \propto \mathcal{N}(\mu_l=0, \sigma_l)[A(l_o, l_v, x_t)]\\
p(r | x_t) \propto \mathcal{N}(\mu_r=0, \sigma_r)[A(r_o, r_v, x_t)]
\end{align}

If the person's arm is more than a certain angle away from the table,
we assume they are referring to none of the objects, and perform an
update.  As a result, these gestures do not effect the robot's
estimate of the objects being referenced.\\\\
\noindent{\bf Head Pose}

Head pose is modeled in the same manner as arm gestures.
\begin{align}
p(h | x_t) \propto \mathcal{N}(\mu_h=0, \sigma_h)[A(h_o, h_v, x_t)]
\end{align}


\noindent{\bf Speech}

We model speech with a unigram model, namely we
take each word in a given speech input and calculate the probability that, given the state, that word would have been spoken.
\begin{align}
p(s |x_t) = \displaystyle \prod_{w \in s} p(w | x_t)
\end{align}
\subsubsection{Null Words and Gesture}

To account for continuous gesture and non-continuous speech input, we
have both null poses and speech.  When no words are spoken, we assume
a null word which has a uniform distribution over the objects.  This
effect means that spoken words cause a discrete bump in probability
according to the language model, which then decays over time. While
gesture remains a continuous input throughout the entire interaction,
many gestures have little or no meaning. Originally, we approached this problem by 
also using the position of the users feet. If a gesture was closer to the feet than any object, we treated
that gesture as uniform across all objects. While this is the method we used for the user studies, we later
switched to using a min probability, effectively using the Gaussian only if the object was within a certain angle,
and applying a min probability otherwise. This helped consistently deal with underflow issues and normalization
of tiny probabilities.
\subsubsection{Model Parameters}
We tuned model parameters by hand.  We considered collecting and
annotating a data set to train the model parameters, but we found our
initial process to be quite accurate. We generated the language model
by hand, adding to it based on results of our pilot
studies. After our initial tuning, we fixed
 model parameters for the user studies.

In our experiments, we had the following parameters: the transition
probability, $c$, was 0.9995. We set this parameter to give an object
that has 100\% confidence an approximately 10\% drop in confidence per
second with all null observations.  Standard deviation for the
Gaussian used to model probability of gesture, $\sigma_l$, $\sigma_r$,
and $\sigma_h$ were 1.0 radians. We found that this standard deviation
allowed for accurate pointing, without skewing the probabilities
during an arm swing.  The language model consisted of 16 unique words,
containing common descriptors for the objects such as ``bowl,''
``spoon,'' ``metal,'' ``shiny,'' etc. It also included words that were
commonly misinterpreted by the speech recognition system, such as
``bull'' when the user was requesting a bowl.

\begin{algorithm}
    \DontPrintSemicolon
    \KwIn{$bel(x_{t-1}), z_t$}
    \BlankLine
    \KwOut{$bel(x_t)$}
    \BlankLine
    \For{ $x_t$} {
      $\bar{bel}(x_t) = \displaystyle\sum_{x_{t-1}} p(x_t|x_{t-1})*bel(x_{t-1})$
      \BlankLine
      \textbf{if not} is\_null\_gesture(l)
      \BlankLine
      \Indp$\bar{bel}(x_t) = p(l | x_t) *  \bar{bel}(x_t)$
      \BlankLine
      \Indm\textbf{if not} is\_null\_gesture(r)
      \BlankLine
      \Indp$\bar{bel}(x_t) = p(r | x_t) *  \bar{bel}(x_t)$
      \BlankLine
      \Indm\textbf{if not} is\_null\_gesture(h)
      \BlankLine
      \Indp$\bar{bel}(x_t) = p(h | x_t) *  \bar{bel}(x_t)$
      \BlankLine
      \Indm\For{$w \in s$}{
      	$\bar{bel}(x_t) = p(w | x_t) *  \bar{bel}(x_t)$
      }
      $bel(x_t) = \bar{bel}(x_t)$

    }
    \BlankLine
\caption{Interactive Bayes Filtering Algorithm} 
\label{alg:algorithm}
\end{algorithm}

Algorithm~\ref{alg:algorithm} shows pseudocode for our approach, while
Figure~\ref{fig:cartoon} shows an example interaction using this algorithm.
The person's gesture is ambiguous, and the system initially infers an
approximately bimodal distribution two objects.  The robot indicates it has not understood by showing a
confused face.  This reaction elicits a disambiguating response from
the person, who says, ``???''  The model incorporates
information from language and infers the person is referring to the
????.  The robot indicates it has understood with a facial
expression and by pointing to the correct object.  It is important to note that
neither the gesture or the speech uniquely distinguished the object to the robot,
but by the nature of the Bayes Filter approach, the two observatons at separate times
are combined to yield the true result.

Although in this example we are demonstrating the approach at two
specific timesteps, the system is updating its distribution
continuously, enabling it to fuse language and gesture as it occurs
and quickly updating in response to new input from the person, verbal
or nonverbal.  Our approach runs at 14Hz, including a 30Hz sleep
cycle, on an Asus machine with 8 2.4 GHz Intel Cores that is also
performing all perceptual and network processing. This is used in conjunction with the Baxter Robot and a Kinect V1.
\subsection*{User Studies}
We ran 65 user trials, consisting of 13 participants running 5 trials each to measure the algorithm's accuracy in estimating a user's referenced object. Each subject stood in front of a table with four objects arranged to form a square, as appears in Figure ~\ref{fig:corpus_scene}. The four objects used were a metal spoon, a plastic spoon, a metal bowl, and a plasic bowl. This was designed so that few single word descriptions would uniquely differentiate all the objects. We instructed each participant to ask for the object indicated with a laser pointer in whatever way felt most natural, using any combination of language or gesture. The participants also wore a microphone to pick up quality audio for transcription. We used the HTML5 Webkit Audio API in conjuction with Google Chrome for speech transcription. This package supports incremental output as recognition proceeds, allowing us to update the model each time a new word is received, fitting in well with our continuous model.

Results showing the percent of the time the estimated most likely
object was the true object appear in Table~\ref{table:real_results}
with 95\% confidence intervals.  During a typical trial, the model
starts out approximately uniform or unimodal on the previous object
(we did not reset the model between trials for the same user) As the subject points and
talks, the model quickly converges to the correct object.  Our first
set of results give a sense of how quickly the model converges by measuring total time correct.

To assess overall accuracy, we report the system's accuracy at the end
of a trial in \ref{table:end_real}.  Multimodal accuracy with language
and gesture is more than 90\%, demonstrating that our approach is able
to quickly and accurately interpret unscripted language and gesture
produced by a user.  We found that our head pose estimator was quite
inaccurate, performing slightly below random.  Thus overall
results that include head pose perform worse than language and
gesture.  We believe this effect has several sources, including marginal head movement when objects are relatively close together, as well as general inaccuracy in head rotation tracking. 

The difference in accuracy between gesture alone and the multimodal output is not as large as one might expect. This is in part caused by the small delay in speech recognition software as opposed to the instantaneous gesture input. Additionally, many subjects, when told they could use gestures, leaned towards relying almost entirely on gesticulation. There were some users, however, who relied on an equal mix of both, and showed large leaps in accuracy between arms and multimodal. The most extreme example is of a user who, over their five trials, achieved only 45.5\% accuracy with arms alone and 42.2\% with speech alone, yet managed to achieve 85.7\% multimodal accuracy, only 2 percentage points away from the sum of the two probabilities, showing the ease at which alternating speech and gesture can give incredibly accurate results overall. While a combination of ambiguous speech and gesture such as "that spoon" followed by a gesture would be more accurate than just a gesture, we found that most test subjects either spoke with complete ambiguity or none, using phrases either of the form "hand me that thing" or "hand me the silver spoon". Therefore we were unable to fully test this hypothesis.

\begin{table}
\caption{Real-world Results\label{table:real_results}}
\centering
\begin{tabular}{lr}
\toprule
Random & 25\%\\
Language only &  32.4\% +/- 10\%\\
Gesture only  &  73.12\% +/- 9\%\\
Head only     &  21.67\% +/- 10\%\\
Multimodal (Language and Gesture) & {\bf 81.99\% +/- 5.5\%}\\
Multimodal (All) &  64.84\% +/- 8\%\\
\bottomrule
\end{tabular}
\end{table}
\begin{table}
\caption{Real-world Results (End of Interaction)\label{table:end_real}}
\centering
\begin{tabular}{lr}
\toprule
Random & 25\%\\
Language only &  46.15\%\\
Gesture only  &  80.0\%\\
Head only     & 18.46\%\\
Multimodal (Language and Gesture) & {\bf 90.77\%}\\
Multimodal (All) &  61.54\%\\
\bottomrule
\end{tabular}
\end{table}
\section{CONCLUSION}

We have demonstrated a Bayes' filtering approach to interpreting a
person's multimodal language and gesture references to objects
continuously in real time.  Our approach enables a robot to understand
a person's references to objects in the real world. This provides
a novel method of continuous interpretation and understanding 
of human referring expressions.

In the future, we plan to expand this ability to facilitate meaningful
responses from the robotic assistant to help communicate a lack of understanding.
To do this we plan on using the Bayes Filter as the basis of a Partially Observable
Markov Decision Process, which allows for actions, where a Bayes Filter does not.

\end{document}